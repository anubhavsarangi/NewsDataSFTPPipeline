{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca22b62f",
   "metadata": {},
   "source": [
    "#### Install and import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567e5e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea610ee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check for spark dependency (ONLY for LOCAL SPARK INSTALLATION)\n",
    "\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53422beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import PySpark dependencies\n",
    "from pyspark.sql.functions import split, col, regexp_replace, trim\n",
    "\n",
    "import pyspark \n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a69016e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paramiko\n",
    "import yaml\n",
    "import os\n",
    "import shutil\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884e6b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Widen the display of Pandas DataFrame output\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6ebdf3",
   "metadata": {},
   "source": [
    "#### Read SFTP credentials from YAML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a74afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sftp_cred = yaml.load(open('sftp_credentials.yml'), yaml.FullLoader)\n",
    "hostname = sftp_cred['hostname']\n",
    "username = sftp_cred['username']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607a4f45",
   "metadata": {},
   "source": [
    "#### Establish Connection to SFTP Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66e017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = paramiko.RSAKey.from_private_key_file(\"./sftp_ssh.pem\")\n",
    "ssh = paramiko.SSHClient()\n",
    "ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "\n",
    "print(\"Connecting to host...\")\n",
    "ssh.connect(hostname = hostname, username = username, pkey = k)\n",
    "\n",
    "print(\"Authenticated.\\nStarting the session...\")\n",
    "sftp = ssh.open_sftp()\n",
    "print(\"Connected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592ece0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get list of files in the sftp directory\n",
    "files = sftp.listdir()\n",
    "print(str(len(files)) + \" Files to be processed:\")\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386ac2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate and create a folders to store raw and curated data \n",
    "if os.path.exists('./tmp/'):\n",
    "    shutil.rmtree('./tmp/')\n",
    "    os.mkdir('./tmp/')\n",
    "    os.mkdir('./tmp/raw/')\n",
    "    os.mkdir('./tmp/curated/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d96e88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(filename):\n",
    "    '''Takes in filename to be processed\n",
    "  \n",
    "    Transforms the data as follows:\n",
    "    - Updates column datatypes\n",
    "    - Column enrichments\n",
    "    \n",
    "    Loads data to '/tmp/curated/' local folder \n",
    "  \n",
    "    Parameters:\n",
    "    filename (string): Name of the file\n",
    "  \n",
    "    '''\n",
    "    \n",
    "    #Read CSV\n",
    "    df = spark.read.option(\"header\", \"true\")\\\n",
    "                .option(\"quote\", \"\\\"\")\\\n",
    "                .option(\"multiline\",\"true\")\\\n",
    "                .csv(\"./tmp/raw/\" + filename, header=True)\n",
    "    \n",
    "    #Set SparkSession time zone as UTC\n",
    "    spark.conf.set('spark.sql.session.timeZone', 'UTC')\n",
    "    \n",
    "    #Convert string attributes to timestamp\n",
    "    df = df.withColumn(\"STORY_DATE_TIME\",df['STORY_DATE_TIME']\\\n",
    "                       .cast('timestamp'))\n",
    "    df = df.withColumn(\"TAKE_DATE_TIME\",df['TAKE_DATE_TIME']\\\n",
    "                       .cast('timestamp'))\n",
    "    \n",
    "    #Remove '\\n\\r' special characters from the text attributes\n",
    "    df = df.withColumn(\"ACCUMULATED_STORY_TEXT\", regexp_replace(col(\"ACCUMULATED_STORY_TEXT\"), \"[\\n\\r]\", \" \"))\n",
    "    df = df.withColumn(\"TAKE_TEXT\", regexp_replace(col(\"TAKE_TEXT\"), \"[\\n\\r]\", \" \"))\n",
    "    \n",
    "    #Extract Keywords from the text  \n",
    "    df = df.withColumn('ACCUMULATED_STORY_TEXT_KEYWORDS', trim(split(df['ACCUMULATED_STORY_TEXT'], 'Keywords:')\\\n",
    "                   .getItem(1)))\n",
    "    df = df.withColumn('TAKE_TEXT_KEYWORDS', trim(split(df['TAKE_TEXT'], 'Keywords:')\\\n",
    "                   .getItem(1)))\n",
    "    \n",
    "    #Reorder columns in the data set\n",
    "    df = df.select(\"DATE\",\"TIME\",\"UNIQUE_STORY_INDEX\",\"EVENT_TYPE\", \"PNAC\", \"STORY_DATE_TIME\", \"TAKE_DATE_TIME\",\\\n",
    "                   \"HEADLINE_ALERT_TEXT\", \"ACCUMULATED_STORY_TEXT\", \"ACCUMULATED_STORY_TEXT_KEYWORDS\", \"TAKE_TEXT\",\\\n",
    "                   \"TAKE_TEXT_KEYWORDS\", \"PRODUCTS\", \"RELATED_RICS\", \"NAMED_ITEMS\", \"HEADLINE_SUBTYPE\", \"STORY_TYPE\",\\\n",
    "                   \"TABULAR_FLAG\", \"ATTRIBUTION\", \"LANGUAGE\")\n",
    "    \n",
    "    #Export data set to csv format\n",
    "    df.toPandas().to_csv(\"./tmp/curated/\" + filename[:-3], quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8033293b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Fetch files from SFTP Server\n",
    "for f in files[0:1]:\n",
    "    print(\"Downloading \" + f + \" ...\")\n",
    "    sftp.get(f, \"./tmp/raw/\" + f)\n",
    "    print(\"Raw data downloaded.\\nTransforming data...\")\n",
    "    transform_data(f)\n",
    "    print(\"Curated data loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cd2176",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", \"true\")\\\n",
    "                .option(\"quote\", \"\\\"\")\\\n",
    "                .option(\"multiline\",\"true\")\\\n",
    "                .csv(\"./tmp/raw/2013-07-01.csv.gz\", header=True)\n",
    "    \n",
    "    #Set SparkSession time zone as UTC\n",
    "spark.conf.set('spark.sql.session.timeZone', 'UTC')\n",
    "    \n",
    "    #Convert string attributes to timestamp\n",
    "df = df.withColumn(\"STORY_DATE_TIME\",df['STORY_DATE_TIME']\\\n",
    "                   .cast('timestamp'))\n",
    "df = df.withColumn(\"TAKE_DATE_TIME\",df['TAKE_DATE_TIME']\\\n",
    "                   .cast('timestamp'))\n",
    "\n",
    "#Remove '\\n\\r' special characters from the text attributes\n",
    "df = df.withColumn(\"ACCUMULATED_STORY_TEXT\", regexp_replace(col(\"ACCUMULATED_STORY_TEXT\"), \"[\\n\\r]\", \" \"))\n",
    "df = df.withColumn(\"TAKE_TEXT\", regexp_replace(col(\"TAKE_TEXT\"), \"[\\n\\r]\", \" \"))\n",
    "\n",
    "#Extract Keywords from the text  \n",
    "df = df.withColumn('ACCUMULATED_STORY_TEXT_KEYWORDS', trim(split(df['ACCUMULATED_STORY_TEXT'], 'Keywords:')\\\n",
    "               .getItem(1)))\n",
    "df = df.withColumn('TAKE_TEXT_KEYWORDS', trim(split(df['TAKE_TEXT'], 'Keywords:')\\\n",
    "               .getItem(1)))\n",
    "\n",
    "#Reorder columns in the data set\n",
    "df = df.select(\"DATE\",\"TIME\",\"UNIQUE_STORY_INDEX\",\"EVENT_TYPE\", \"PNAC\", \"STORY_DATE_TIME\", \"TAKE_DATE_TIME\",\\\n",
    "               \"HEADLINE_ALERT_TEXT\", \"ACCUMULATED_STORY_TEXT\", \"ACCUMULATED_STORY_TEXT_KEYWORDS\", \"TAKE_TEXT\",\\\n",
    "               \"TAKE_TEXT_KEYWORDS\", \"PRODUCTS\", \"RELATED_RICS\", \"NAMED_ITEMS\", \"HEADLINE_SUBTYPE\", \"STORY_TYPE\",\\\n",
    "               \"TABULAR_FLAG\", \"ATTRIBUTION\", \"LANGUAGE\")\n",
    "\n",
    "#Export data set to csv format\n",
    "#df.toPandas().to_csv(\"./tmp/curated/\" + filename[:-3], quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baf342c",
   "metadata": {},
   "source": [
    "### Future Scope\n",
    "\n",
    "- Column enrichments like extracing Page Editor, Reporting by, Editing by, etc. (from \"ACCUMULATED_STORY_TEXT\" or \"TAKE_TEXT\" attribute) which would simplify querying and searching through the data.\n",
    "- Few attributes have records containing articles and tables, these could be stored in document databases.\n",
    "- The data contains text from multiple languages - Arabic, Spanish, etc.; these could be translated using APIs and standardized to English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57278cdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
